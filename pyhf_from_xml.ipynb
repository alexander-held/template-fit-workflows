{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read xml workspace and fit using pyhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "import pyhf.readxml\n",
    "import json\n",
    "\n",
    "pyhf.set_backend(\"numpy\", pyhf.optimize.minuit_optimizer(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_xml_path = \"TRExFitter/minimal_example/RooStats/minimal_example.xml\"\n",
    "parent_folder = \"TRExFitter/\"\n",
    "json_path     = \"workspace_from_xml.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### translate the `xml` to python and dump it to a `json` file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = pyhf.readxml.parse(main_xml_path, parent_folder, track_progress=False)\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(spec, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load spec from file again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, \"r\") as f:\n",
    "    spec = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the relevant pyhf objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = pyhf.Workspace(spec)\n",
    "model = workspace.model()\n",
    "data = workspace.data(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform an unconstrained fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "| FCN = 21.61                   |     Ncalls=212 (212 total)     |\n",
      "| EDM = 0.000108 (Goal: 1E-05)  |            up = 1.0            |\n",
      "------------------------------------------------------------------\n",
      "|  Valid Min.   | Valid Param.  | Above EDM | Reached call limit |\n",
      "------------------------------------------------------------------\n",
      "|     True      |     True      |   False   |       False        |\n",
      "------------------------------------------------------------------\n",
      "| Hesse failed  |   Has cov.    | Accurate  | Pos. def. | Forced |\n",
      "------------------------------------------------------------------\n",
      "|     False     |     True      |   True    |   True    | False  |\n",
      "------------------------------------------------------------------\n",
      "CPU times: user 50.1 ms, sys: 4.53 ms, total: 54.6 ms\n",
      "Wall time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = pyhf.infer.mle.fit(data, model, return_uncertainties=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lumi: 1.00210226827377 +/- 0.01929133949918138\n",
      "staterror_Signal_region[bin_0]: 1.0003425715548206 +/- 0.005807817286532124\n",
      "staterror_Signal_region[bin_1]: 0.9988215875033956 +/- 0.0056055039396046835\n",
      "staterror_Signal_region[bin_2]: 1.0034524200724302 +/- 0.005095426443941953\n",
      "staterror_Signal_region[bin_3]: 0.9975282708630174 +/- 0.006000955883891657\n",
      "staterror_Signal_region[bin_4]: 0.9989829541808631 +/- 0.007892029640412968\n",
      "staterror_Signal_region[bin_5]: 0.9973982717169335 +/- 0.011986329787382344\n",
      "Luminosity: 0.00024602355778569063 +/- 0.9111326823379149\n",
      "Signal_norm: 1.1863179067062717 +/- 0.03731826916514769\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_names(model):\n",
    "    labels = []\n",
    "    for parname in model.config.par_order:\n",
    "        for i_par in range(model.config.param_set(parname).n_parameters):\n",
    "            labels.append(\n",
    "                \"{}[bin_{}]\".format(parname, i_par)\n",
    "                if model.config.param_set(parname).n_parameters > 1\n",
    "                else parname\n",
    "            )\n",
    "    return labels\n",
    "\n",
    "bestfit = result[:, 0]\n",
    "uncertainty = result[:, 1]\n",
    "labels = get_parameter_names(model)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label}: {bestfit[i]} +/- {uncertainty[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `lumi` and `Luminosity` both show up here and are highly anti-correlated. While `lumi` is a built-in way that HistFactory provides for a luminosity uncertainty, TRExFitter does not make use of it. Instead, a more configurable normalization factor is manually implemented in the TRExFitter configuration. While the `lumi` parameter is set to constant in `minimal_example.xml` (via `<ParamSetting Const=\"True\">Lumi</ParamSetting>`), this information is not yet propagated automatically through `pyhf` to the minimizer. See also https://github.com/scikit-hep/pyhf/pull/846."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyhf)",
   "language": "python",
   "name": "pyhf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
